{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set resnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# code from https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10/blob/master/model.py\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.down_sample = down_sample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, num_classes=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # feature map size = 112x112x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 56x56x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 28x28x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        self.avg_pool = nn.MaxPool2d(28, stride=1)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "        \n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "            \n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    block = ResidualBlock\n",
    "    # total number of layers if 6n + 2. if n is 5 then the depth of network is 32.\n",
    "    model = ResNet(3, block) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/moskomule/senet.pytorch/blob/master/senet/se_resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SELayer(planes * 4, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def se_resnet18(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet34(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet50(num_classes=7, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_state_dict_from_url(\n",
    "            \"https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet101(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet152(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "cropped_image_list = []\n",
    "label_list = []\n",
    "img_path = '../images/'\n",
    "\n",
    "with open('result.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomCrop(112, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "#number of train set and test set\n",
    "num_data=len(data.index)\n",
    "num_test_data = int(num_data/10)\n",
    "num_train_data = num_data - num_test_data\n",
    "\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "class_dict = {'JM': 0,'JN':1, 'JH':2, 'JK':3, 'RM':4, 'VV':5, 'SG':6 }\n",
    "  \n",
    "for i in data.index:\n",
    "    img = Image.open(img_path+data[0][i])\n",
    "    cropped_image_list.append(img)\n",
    "    \n",
    "    label = class_dict[data[1][i]]\n",
    "    label_list.append(label)\n",
    "    \n",
    "\n",
    "#dataset making by random select\n",
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "#random select test data index\n",
    "rand = np.random.choice(np.arange(num_data), num_test_data, replace=False)\n",
    "\n",
    "# class dataset() and __init__() looks good for calling and managing self variables\n",
    "# but This time, tried to make simple without class\n",
    "\n",
    "count_test=0\n",
    "count_train=0\n",
    "for i in np.arange(num_data):\n",
    "    if i in rand:\n",
    "        img = transforms_test(cropped_image_list[i])\n",
    "        dataset_test.append([img,label_list[i]])\n",
    "    else:\n",
    "        img = transforms_train(cropped_image_list[i])\n",
    "        dataset_train.append([img,label_list[i]])\n",
    "        \n",
    "#dataset_test = np.asarray(dataset_test)\n",
    "#dataset_train = np.asarray(dataset_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss\n",
    "https://github.com/foamliu/InsightFace-v2/blob/master/focal_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Making model..\n",
      "The number of parameters of model is 11267143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2b666c82231d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[0mglobal_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m             \u001b[0mbest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best test accuracy is '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-2b666c82231d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, global_steps)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\aix360\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m                     \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "exp_id=0\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "batch_size_test=2\n",
    "num_worker=1\n",
    "resume = None\n",
    "logdir = './output/'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=num_worker)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size_test, \n",
    "                         shuffle=False, num_workers=num_worker)\n",
    "\n",
    "# bts class\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "\n",
    "print('==> Making model..')\n",
    "\n",
    "#net = resnet()\n",
    "net = se_resnet18()\n",
    "net = net.to(device)\n",
    "num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('The number of parameters of model is', num_params)\n",
    "# print(net)\n",
    "\n",
    "if resume is not None:\n",
    "    checkpoint = torch.load('../save_model/' + str(exp_id))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma=2.0).to(device)\n",
    "#criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, \n",
    "#                      eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "decay_epoch = [2400, 3600]\n",
    "step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, \n",
    "                                 milestones=decay_epoch, gamma=0.1)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "\n",
    "def train(epoch, global_steps):\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        global_steps += 1\n",
    "        step_lr_scheduler.step()\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    acc = 100 * correct / total\n",
    "    print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(train_loader), train_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/train error', 100 - acc, global_steps)\n",
    "    return global_steps\n",
    "\n",
    "\n",
    "def test(epoch, best_acc, global_steps):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print('test epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(test_loader), test_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/test error', 100 - acc, global_steps)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        print('==> Saving model..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('../save_model'):\n",
    "            os.mkdir('../save_model')\n",
    "        torch.save(state, '../save_model/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    best_acc = 0\n",
    "    epoch = 0\n",
    "    global_steps = 0\n",
    "    \n",
    "    if resume is not None:\n",
    "        test(epoch=0, best_acc=0)\n",
    "    else:\n",
    "        while True:\n",
    "            epoch += 1\n",
    "            global_steps = train(epoch, global_steps)\n",
    "            best_acc = test(epoch, best_acc, global_steps)\n",
    "            print('best test accuracy is ', best_acc)\n",
    "            \n",
    "            if global_steps >= 4800:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set resnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# code from https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10/blob/master/model.py\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.down_sample = down_sample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, num_classes=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # feature map size = 112x112x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 56x56x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 28x28x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        self.avg_pool = nn.MaxPool2d(28, stride=1)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "        \n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "            \n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    block = ResidualBlock\n",
    "    # total number of layers if 6n + 2. if n is 5 then the depth of network is 32.\n",
    "    model = ResNet(3, block) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/moskomule/senet.pytorch/blob/master/senet/se_resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SELayer(planes * 4, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def se_resnet18(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet34(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet50(num_classes=7, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_state_dict_from_url(\n",
    "            \"https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet101(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet152(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "cropped_image_list = []\n",
    "label_list = []\n",
    "img_path = './cropped/'\n",
    "\n",
    "with open('result.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomCrop(112, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "#number of train set and test set\n",
    "num_data=len(data.index)\n",
    "num_test_data = int(num_data/10)\n",
    "num_train_data = num_data - num_test_data\n",
    "\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "class_dict = {'JM': 0,'JN':1, 'JH':2, 'JK':3, 'RM':4, 'VV':5, 'SG':6 }\n",
    "  \n",
    "for i in data.index:\n",
    "    img = Image.open(img_path+data[0][i])\n",
    "    cropped_image_list.append(img)\n",
    "    \n",
    "    label = class_dict[data[1][i]]\n",
    "    label_list.append(label)\n",
    "    \n",
    "\n",
    "#dataset making by random select\n",
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "#random select test data index\n",
    "rand = np.random.choice(np.arange(num_data), num_test_data, replace=False)\n",
    "\n",
    "# class dataset() and __init__() looks good for calling and managing self variables\n",
    "# but This time, tried to make simple without class\n",
    "\n",
    "count_test=0\n",
    "count_train=0\n",
    "for i in np.arange(num_data):\n",
    "    if i in rand:\n",
    "        img = transforms_test(cropped_image_list[i])\n",
    "        dataset_test.append([img,label_list[i]])\n",
    "    else:\n",
    "        img = transforms_train(cropped_image_list[i])\n",
    "        dataset_train.append([img,label_list[i]])\n",
    "        \n",
    "#dataset_test = np.asarray(dataset_test)\n",
    "#dataset_train = np.asarray(dataset_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss\n",
    "https://github.com/foamliu/InsightFace-v2/blob/master/focal_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "exp_id=0\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "batch_size_test=2\n",
    "num_worker=1\n",
    "resume = None\n",
    "logdir = './output/'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=num_worker)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size_test, \n",
    "                         shuffle=False, num_workers=num_worker)\n",
    "\n",
    "# bts class\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "\n",
    "print('==> Making model..')\n",
    "\n",
    "#net = resnet()\n",
    "net = se_resnet18()\n",
    "net = net.to(device)\n",
    "num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('The number of parameters of model is', num_params)\n",
    "# print(net)\n",
    "\n",
    "if resume is not None:\n",
    "    checkpoint = torch.load('./save_model/' + str(exp_id))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma=2.0).to(device)\n",
    "#criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, \n",
    "#                      eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "decay_epoch = [2400, 3600]\n",
    "step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, \n",
    "                                 milestones=decay_epoch, gamma=0.1)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "\n",
    "def train(epoch, global_steps):\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        global_steps += 1\n",
    "        step_lr_scheduler.step()\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    acc = 100 * correct / total\n",
    "    print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(train_loader), train_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/train error', 100 - acc, global_steps)\n",
    "    return global_steps\n",
    "\n",
    "\n",
    "def test(epoch, best_acc, global_steps):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print('test epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(test_loader), test_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/test error', 100 - acc, global_steps)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        print('==> Saving model..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('save_model'):\n",
    "            os.mkdir('save_model')\n",
    "        torch.save(state, './save_model/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    best_acc = 0\n",
    "    epoch = 0\n",
    "    global_steps = 0\n",
    "    \n",
    "    if resume is not None:\n",
    "        test(epoch=0, best_acc=0)\n",
    "    else:\n",
    "        while True:\n",
    "            epoch += 1\n",
    "            global_steps = train(epoch, global_steps)\n",
    "            best_acc = test(epoch, best_acc, global_steps)\n",
    "            print('best test accuracy is ', best_acc)\n",
    "            \n",
    "            if global_steps >= 4800:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set resnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# code from https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10/blob/master/model.py\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.down_sample = down_sample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, num_classes=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # feature map size = 112x112x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 56x56x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 28x28x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        self.avg_pool = nn.MaxPool2d(28, stride=1)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "        \n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "            \n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    block = ResidualBlock\n",
    "    # total number of layers if 6n + 2. if n is 5 then the depth of network is 32.\n",
    "    model = ResNet(3, block) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/moskomule/senet.pytorch/blob/master/senet/se_resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SELayer(planes * 4, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def se_resnet18(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet34(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet50(num_classes=7, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_state_dict_from_url(\n",
    "            \"https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet101(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet152(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "cropped_image_list = []\n",
    "label_list = []\n",
    "img_path = './cropped/'\n",
    "\n",
    "with open('result.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomCrop(112, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "#number of train set and test set\n",
    "num_data=len(data.index)\n",
    "num_test_data = int(num_data/10)\n",
    "num_train_data = num_data - num_test_data\n",
    "\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "class_dict = {'JM': 0,'JN':1, 'JH':2, 'JK':3, 'RM':4, 'VV':5, 'SG':6 }\n",
    "  \n",
    "for i in data.index:\n",
    "    img = Image.open(img_path+data[0][i])\n",
    "    cropped_image_list.append(img)\n",
    "    \n",
    "    label = class_dict[data[1][i]]\n",
    "    label_list.append(label)\n",
    "    \n",
    "\n",
    "#dataset making by random select\n",
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "#random select test data index\n",
    "rand = np.random.choice(np.arange(num_data), num_test_data, replace=False)\n",
    "\n",
    "# class dataset() and __init__() looks good for calling and managing self variables\n",
    "# but This time, tried to make simple without class\n",
    "\n",
    "count_test=0\n",
    "count_train=0\n",
    "for i in np.arange(num_data):\n",
    "    if i in rand:\n",
    "        img = transforms_test(cropped_image_list[i])\n",
    "        dataset_test.append([img,label_list[i]])\n",
    "    else:\n",
    "        img = transforms_train(cropped_image_list[i])\n",
    "        dataset_train.append([img,label_list[i]])\n",
    "        \n",
    "#dataset_test = np.asarray(dataset_test)\n",
    "#dataset_train = np.asarray(dataset_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss\n",
    "https://github.com/foamliu/InsightFace-v2/blob/master/focal_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "exp_id=0\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "batch_size_test=2\n",
    "num_worker=1\n",
    "resume = None\n",
    "logdir = './output/'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=num_worker)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size_test, \n",
    "                         shuffle=False, num_workers=num_worker)\n",
    "\n",
    "# bts class\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "\n",
    "print('==> Making model..')\n",
    "\n",
    "#net = resnet()\n",
    "net = se_resnet18()\n",
    "net = net.to(device)\n",
    "num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('The number of parameters of model is', num_params)\n",
    "# print(net)\n",
    "\n",
    "if resume is not None:\n",
    "    checkpoint = torch.load('./save_model/' + str(exp_id))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma=2.0).to(device)\n",
    "#criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, \n",
    "#                      eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "decay_epoch = [2400, 3600]\n",
    "step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, \n",
    "                                 milestones=decay_epoch, gamma=0.1)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "\n",
    "def train(epoch, global_steps):\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        global_steps += 1\n",
    "        step_lr_scheduler.step()\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    acc = 100 * correct / total\n",
    "    print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(train_loader), train_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/train error', 100 - acc, global_steps)\n",
    "    return global_steps\n",
    "\n",
    "\n",
    "def test(epoch, best_acc, global_steps):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print('test epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(test_loader), test_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/test error', 100 - acc, global_steps)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        print('==> Saving model..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('save_model'):\n",
    "            os.mkdir('save_model')\n",
    "        torch.save(state, './save_model/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    best_acc = 0\n",
    "    epoch = 0\n",
    "    global_steps = 0\n",
    "    \n",
    "    if resume is not None:\n",
    "        test(epoch=0, best_acc=0)\n",
    "    else:\n",
    "        while True:\n",
    "            epoch += 1\n",
    "            global_steps = train(epoch, global_steps)\n",
    "            best_acc = test(epoch, best_acc, global_steps)\n",
    "            print('best test accuracy is ', best_acc)\n",
    "            \n",
    "            if global_steps >= 4800:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set resnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# code from https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10/blob/master/model.py\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.down_sample = down_sample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, num_classes=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # feature map size = 112x112x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 56x56x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 28x28x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        self.avg_pool = nn.MaxPool2d(28, stride=1)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "        \n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "            \n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    block = ResidualBlock\n",
    "    # total number of layers if 6n + 2. if n is 5 then the depth of network is 32.\n",
    "    model = ResNet(3, block) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/moskomule/senet.pytorch/blob/master/senet/se_resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SELayer(planes * 4, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def se_resnet18(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet34(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet50(num_classes=7, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_state_dict_from_url(\n",
    "            \"https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet101(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet152(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "cropped_image_list = []\n",
    "label_list = []\n",
    "img_path = './cropped/'\n",
    "\n",
    "with open('result.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomCrop(112, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "#number of train set and test set\n",
    "num_data=len(data.index)\n",
    "num_test_data = int(num_data/10)\n",
    "num_train_data = num_data - num_test_data\n",
    "\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "class_dict = {'JM': 0,'JN':1, 'JH':2, 'JK':3, 'RM':4, 'VV':5, 'SG':6 }\n",
    "  \n",
    "for i in data.index:\n",
    "    img = Image.open(img_path+data[0][i])\n",
    "    cropped_image_list.append(img)\n",
    "    \n",
    "    label = class_dict[data[1][i]]\n",
    "    label_list.append(label)\n",
    "    \n",
    "\n",
    "#dataset making by random select\n",
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "#random select test data index\n",
    "rand = np.random.choice(np.arange(num_data), num_test_data, replace=False)\n",
    "\n",
    "# class dataset() and __init__() looks good for calling and managing self variables\n",
    "# but This time, tried to make simple without class\n",
    "\n",
    "count_test=0\n",
    "count_train=0\n",
    "for i in np.arange(num_data):\n",
    "    if i in rand:\n",
    "        img = transforms_test(cropped_image_list[i])\n",
    "        dataset_test.append([img,label_list[i]])\n",
    "    else:\n",
    "        img = transforms_train(cropped_image_list[i])\n",
    "        dataset_train.append([img,label_list[i]])\n",
    "        \n",
    "#dataset_test = np.asarray(dataset_test)\n",
    "#dataset_train = np.asarray(dataset_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss\n",
    "https://github.com/foamliu/InsightFace-v2/blob/master/focal_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "exp_id=0\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "batch_size_test=2\n",
    "num_worker=1\n",
    "resume = None\n",
    "logdir = './output/'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=num_worker)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size_test, \n",
    "                         shuffle=False, num_workers=num_worker)\n",
    "\n",
    "# bts class\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "\n",
    "print('==> Making model..')\n",
    "\n",
    "#net = resnet()\n",
    "net = se_resnet18()\n",
    "net = net.to(device)\n",
    "num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('The number of parameters of model is', num_params)\n",
    "# print(net)\n",
    "\n",
    "if resume is not None:\n",
    "    checkpoint = torch.load('./save_model/' + str(exp_id))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma=2.0).to(device)\n",
    "#criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, \n",
    "#                      eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "decay_epoch = [2400, 3600]\n",
    "step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, \n",
    "                                 milestones=decay_epoch, gamma=0.1)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "\n",
    "def train(epoch, global_steps):\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        global_steps += 1\n",
    "        step_lr_scheduler.step()\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    acc = 100 * correct / total\n",
    "    print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(train_loader), train_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/train error', 100 - acc, global_steps)\n",
    "    return global_steps\n",
    "\n",
    "\n",
    "def test(epoch, best_acc, global_steps):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print('test epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(test_loader), test_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/test error', 100 - acc, global_steps)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        print('==> Saving model..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('save_model'):\n",
    "            os.mkdir('save_model')\n",
    "        torch.save(state, './save_model/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    best_acc = 0\n",
    "    epoch = 0\n",
    "    global_steps = 0\n",
    "    \n",
    "    if resume is not None:\n",
    "        test(epoch=0, best_acc=0)\n",
    "    else:\n",
    "        while True:\n",
    "            epoch += 1\n",
    "            global_steps = train(epoch, global_steps)\n",
    "            best_acc = test(epoch, best_acc, global_steps)\n",
    "            print('best test accuracy is ', best_acc)\n",
    "            \n",
    "            if global_steps >= 4800:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set resnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# code from https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10/blob/master/model.py\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.down_sample = down_sample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, num_classes=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # feature map size = 112x112x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 56x56x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 28x28x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        self.avg_pool = nn.MaxPool2d(28, stride=1)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "        \n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "            \n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    block = ResidualBlock\n",
    "    # total number of layers if 6n + 2. if n is 5 then the depth of network is 32.\n",
    "    model = ResNet(3, block) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/moskomule/senet.pytorch/blob/master/senet/se_resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SELayer(planes * 4, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def se_resnet18(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet34(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet50(num_classes=7, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_state_dict_from_url(\n",
    "            \"https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet101(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet152(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "cropped_image_list = []\n",
    "label_list = []\n",
    "img_path = './cropped/'\n",
    "\n",
    "with open('result.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomCrop(112, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "#number of train set and test set\n",
    "num_data=len(data.index)\n",
    "num_test_data = int(num_data/10)\n",
    "num_train_data = num_data - num_test_data\n",
    "\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "class_dict = {'JM': 0,'JN':1, 'JH':2, 'JK':3, 'RM':4, 'VV':5, 'SG':6 }\n",
    "  \n",
    "for i in data.index:\n",
    "    img = Image.open(img_path+data[0][i])\n",
    "    cropped_image_list.append(img)\n",
    "    \n",
    "    label = class_dict[data[1][i]]\n",
    "    label_list.append(label)\n",
    "    \n",
    "\n",
    "#dataset making by random select\n",
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "#random select test data index\n",
    "rand = np.random.choice(np.arange(num_data), num_test_data, replace=False)\n",
    "\n",
    "# class dataset() and __init__() looks good for calling and managing self variables\n",
    "# but This time, tried to make simple without class\n",
    "\n",
    "count_test=0\n",
    "count_train=0\n",
    "for i in np.arange(num_data):\n",
    "    if i in rand:\n",
    "        img = transforms_test(cropped_image_list[i])\n",
    "        dataset_test.append([img,label_list[i]])\n",
    "    else:\n",
    "        img = transforms_train(cropped_image_list[i])\n",
    "        dataset_train.append([img,label_list[i]])\n",
    "        \n",
    "#dataset_test = np.asarray(dataset_test)\n",
    "#dataset_train = np.asarray(dataset_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss\n",
    "https://github.com/foamliu/InsightFace-v2/blob/master/focal_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "exp_id=0\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "batch_size_test=2\n",
    "num_worker=1\n",
    "resume = None\n",
    "logdir = './output/'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=num_worker)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size_test, \n",
    "                         shuffle=False, num_workers=num_worker)\n",
    "\n",
    "# bts class\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "\n",
    "print('==> Making model..')\n",
    "\n",
    "#net = resnet()\n",
    "net = se_resnet18()\n",
    "net = net.to(device)\n",
    "num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('The number of parameters of model is', num_params)\n",
    "# print(net)\n",
    "\n",
    "if resume is not None:\n",
    "    checkpoint = torch.load('./save_model/' + str(exp_id))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma=2.0).to(device)\n",
    "#criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, \n",
    "#                      eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "decay_epoch = [2400, 3600]\n",
    "step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, \n",
    "                                 milestones=decay_epoch, gamma=0.1)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "\n",
    "def train(epoch, global_steps):\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        global_steps += 1\n",
    "        step_lr_scheduler.step()\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    acc = 100 * correct / total\n",
    "    print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(train_loader), train_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/train error', 100 - acc, global_steps)\n",
    "    return global_steps\n",
    "\n",
    "\n",
    "def test(epoch, best_acc, global_steps):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print('test epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(test_loader), test_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/test error', 100 - acc, global_steps)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        print('==> Saving model..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('save_model'):\n",
    "            os.mkdir('save_model')\n",
    "        torch.save(state, './save_model/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    best_acc = 0\n",
    "    epoch = 0\n",
    "    global_steps = 0\n",
    "    \n",
    "    if resume is not None:\n",
    "        test(epoch=0, best_acc=0)\n",
    "    else:\n",
    "        while True:\n",
    "            epoch += 1\n",
    "            global_steps = train(epoch, global_steps)\n",
    "            best_acc = test(epoch, best_acc, global_steps)\n",
    "            print('best test accuracy is ', best_acc)\n",
    "            \n",
    "            if global_steps >= 4800:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set resnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# code from https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10/blob/master/model.py\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.down_sample = down_sample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, num_classes=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # feature map size = 112x112x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 56x56x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 28x28x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        self.avg_pool = nn.MaxPool2d(28, stride=1)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "        \n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "            \n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    block = ResidualBlock\n",
    "    # total number of layers if 6n + 2. if n is 5 then the depth of network is 32.\n",
    "    model = ResNet(3, block) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/moskomule/senet.pytorch/blob/master/senet/se_resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SELayer(planes * 4, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def se_resnet18(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet34(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet50(num_classes=7, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_state_dict_from_url(\n",
    "            \"https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet101(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet152(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "cropped_image_list = []\n",
    "label_list = []\n",
    "img_path = './cropped/'\n",
    "\n",
    "with open('result.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomCrop(112, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "#number of train set and test set\n",
    "num_data=len(data.index)\n",
    "num_test_data = int(num_data/10)\n",
    "num_train_data = num_data - num_test_data\n",
    "\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "class_dict = {'JM': 0,'JN':1, 'JH':2, 'JK':3, 'RM':4, 'VV':5, 'SG':6 }\n",
    "  \n",
    "for i in data.index:\n",
    "    img = Image.open(img_path+data[0][i])\n",
    "    cropped_image_list.append(img)\n",
    "    \n",
    "    label = class_dict[data[1][i]]\n",
    "    label_list.append(label)\n",
    "    \n",
    "\n",
    "#dataset making by random select\n",
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "#random select test data index\n",
    "rand = np.random.choice(np.arange(num_data), num_test_data, replace=False)\n",
    "\n",
    "# class dataset() and __init__() looks good for calling and managing self variables\n",
    "# but This time, tried to make simple without class\n",
    "\n",
    "count_test=0\n",
    "count_train=0\n",
    "for i in np.arange(num_data):\n",
    "    if i in rand:\n",
    "        img = transforms_test(cropped_image_list[i])\n",
    "        dataset_test.append([img,label_list[i]])\n",
    "    else:\n",
    "        img = transforms_train(cropped_image_list[i])\n",
    "        dataset_train.append([img,label_list[i]])\n",
    "        \n",
    "#dataset_test = np.asarray(dataset_test)\n",
    "#dataset_train = np.asarray(dataset_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss\n",
    "https://github.com/foamliu/InsightFace-v2/blob/master/focal_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "exp_id=0\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "batch_size_test=2\n",
    "num_worker=1\n",
    "resume = None\n",
    "logdir = './output/'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=num_worker)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size_test, \n",
    "                         shuffle=False, num_workers=num_worker)\n",
    "\n",
    "# bts class\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "\n",
    "print('==> Making model..')\n",
    "\n",
    "#net = resnet()\n",
    "net = se_resnet18()\n",
    "net = net.to(device)\n",
    "num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('The number of parameters of model is', num_params)\n",
    "# print(net)\n",
    "\n",
    "if resume is not None:\n",
    "    checkpoint = torch.load('./save_model/' + str(exp_id))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma=2.0).to(device)\n",
    "#criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, \n",
    "#                      eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "decay_epoch = [2400, 3600]\n",
    "step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, \n",
    "                                 milestones=decay_epoch, gamma=0.1)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "\n",
    "def train(epoch, global_steps):\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        global_steps += 1\n",
    "        step_lr_scheduler.step()\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    acc = 100 * correct / total\n",
    "    print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(train_loader), train_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/train error', 100 - acc, global_steps)\n",
    "    return global_steps\n",
    "\n",
    "\n",
    "def test(epoch, best_acc, global_steps):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print('test epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(test_loader), test_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/test error', 100 - acc, global_steps)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        print('==> Saving model..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('save_model'):\n",
    "            os.mkdir('save_model')\n",
    "        torch.save(state, './save_model/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    best_acc = 0\n",
    "    epoch = 0\n",
    "    global_steps = 0\n",
    "    \n",
    "    if resume is not None:\n",
    "        test(epoch=0, best_acc=0)\n",
    "    else:\n",
    "        while True:\n",
    "            epoch += 1\n",
    "            global_steps = train(epoch, global_steps)\n",
    "            best_acc = test(epoch, best_acc, global_steps)\n",
    "            print('best test accuracy is ', best_acc)\n",
    "            \n",
    "            if global_steps >= 4800:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set resnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# code from https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10/blob/master/model.py\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.down_sample = down_sample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, num_classes=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # feature map size = 112x112x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 56x56x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 28x28x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        self.avg_pool = nn.MaxPool2d(28, stride=1)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "        \n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "            \n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    block = ResidualBlock\n",
    "    # total number of layers if 6n + 2. if n is 5 then the depth of network is 32.\n",
    "    model = ResNet(3, block) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/moskomule/senet.pytorch/blob/master/senet/se_resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SELayer(planes * 4, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def se_resnet18(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet34(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet50(num_classes=7, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_state_dict_from_url(\n",
    "            \"https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet101(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet152(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "cropped_image_list = []\n",
    "label_list = []\n",
    "img_path = './cropped/'\n",
    "\n",
    "with open('result.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomCrop(112, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "#number of train set and test set\n",
    "num_data=len(data.index)\n",
    "num_test_data = int(num_data/10)\n",
    "num_train_data = num_data - num_test_data\n",
    "\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "class_dict = {'JM': 0,'JN':1, 'JH':2, 'JK':3, 'RM':4, 'VV':5, 'SG':6 }\n",
    "  \n",
    "for i in data.index:\n",
    "    img = Image.open(img_path+data[0][i])\n",
    "    cropped_image_list.append(img)\n",
    "    \n",
    "    label = class_dict[data[1][i]]\n",
    "    label_list.append(label)\n",
    "    \n",
    "\n",
    "#dataset making by random select\n",
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "#random select test data index\n",
    "rand = np.random.choice(np.arange(num_data), num_test_data, replace=False)\n",
    "\n",
    "# class dataset() and __init__() looks good for calling and managing self variables\n",
    "# but This time, tried to make simple without class\n",
    "\n",
    "count_test=0\n",
    "count_train=0\n",
    "for i in np.arange(num_data):\n",
    "    if i in rand:\n",
    "        img = transforms_test(cropped_image_list[i])\n",
    "        dataset_test.append([img,label_list[i]])\n",
    "    else:\n",
    "        img = transforms_train(cropped_image_list[i])\n",
    "        dataset_train.append([img,label_list[i]])\n",
    "        \n",
    "#dataset_test = np.asarray(dataset_test)\n",
    "#dataset_train = np.asarray(dataset_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss\n",
    "https://github.com/foamliu/InsightFace-v2/blob/master/focal_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "exp_id=0\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "batch_size_test=2\n",
    "num_worker=1\n",
    "resume = None\n",
    "logdir = './output/'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=num_worker)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size_test, \n",
    "                         shuffle=False, num_workers=num_worker)\n",
    "\n",
    "# bts class\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "\n",
    "print('==> Making model..')\n",
    "\n",
    "#net = resnet()\n",
    "net = se_resnet18()\n",
    "net = net.to(device)\n",
    "num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('The number of parameters of model is', num_params)\n",
    "# print(net)\n",
    "\n",
    "if resume is not None:\n",
    "    checkpoint = torch.load('./save_model/' + str(exp_id))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma=2.0).to(device)\n",
    "#criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, \n",
    "#                      eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "decay_epoch = [2400, 3600]\n",
    "step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, \n",
    "                                 milestones=decay_epoch, gamma=0.1)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "\n",
    "def train(epoch, global_steps):\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        global_steps += 1\n",
    "        step_lr_scheduler.step()\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    acc = 100 * correct / total\n",
    "    print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(train_loader), train_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/train error', 100 - acc, global_steps)\n",
    "    return global_steps\n",
    "\n",
    "\n",
    "def test(epoch, best_acc, global_steps):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print('test epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(test_loader), test_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/test error', 100 - acc, global_steps)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        print('==> Saving model..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('save_model'):\n",
    "            os.mkdir('save_model')\n",
    "        torch.save(state, './save_model/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    best_acc = 0\n",
    "    epoch = 0\n",
    "    global_steps = 0\n",
    "    \n",
    "    if resume is not None:\n",
    "        test(epoch=0, best_acc=0)\n",
    "    else:\n",
    "        while True:\n",
    "            epoch += 1\n",
    "            global_steps = train(epoch, global_steps)\n",
    "            best_acc = test(epoch, best_acc, global_steps)\n",
    "            print('best test accuracy is ', best_acc)\n",
    "            \n",
    "            if global_steps >= 4800:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set resnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# code from https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10/blob/master/model.py\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.down_sample = down_sample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, num_classes=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # feature map size = 112x112x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 56x56x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 28x28x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        self.avg_pool = nn.MaxPool2d(28, stride=1)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "        \n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "            \n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    block = ResidualBlock\n",
    "    # total number of layers if 6n + 2. if n is 5 then the depth of network is 32.\n",
    "    model = ResNet(3, block) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/moskomule/senet.pytorch/blob/master/senet/se_resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SELayer(planes * 4, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def se_resnet18(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet34(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet50(num_classes=7, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_state_dict_from_url(\n",
    "            \"https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet101(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet152(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "cropped_image_list = []\n",
    "label_list = []\n",
    "img_path = './cropped/'\n",
    "\n",
    "with open('result.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomCrop(112, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "#number of train set and test set\n",
    "num_data=len(data.index)\n",
    "num_test_data = int(num_data/10)\n",
    "num_train_data = num_data - num_test_data\n",
    "\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "class_dict = {'JM': 0,'JN':1, 'JH':2, 'JK':3, 'RM':4, 'VV':5, 'SG':6 }\n",
    "  \n",
    "for i in data.index:\n",
    "    img = Image.open(img_path+data[0][i])\n",
    "    cropped_image_list.append(img)\n",
    "    \n",
    "    label = class_dict[data[1][i]]\n",
    "    label_list.append(label)\n",
    "    \n",
    "\n",
    "#dataset making by random select\n",
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "#random select test data index\n",
    "rand = np.random.choice(np.arange(num_data), num_test_data, replace=False)\n",
    "\n",
    "# class dataset() and __init__() looks good for calling and managing self variables\n",
    "# but This time, tried to make simple without class\n",
    "\n",
    "count_test=0\n",
    "count_train=0\n",
    "for i in np.arange(num_data):\n",
    "    if i in rand:\n",
    "        img = transforms_test(cropped_image_list[i])\n",
    "        dataset_test.append([img,label_list[i]])\n",
    "    else:\n",
    "        img = transforms_train(cropped_image_list[i])\n",
    "        dataset_train.append([img,label_list[i]])\n",
    "        \n",
    "#dataset_test = np.asarray(dataset_test)\n",
    "#dataset_train = np.asarray(dataset_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss\n",
    "https://github.com/foamliu/InsightFace-v2/blob/master/focal_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "exp_id=0\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "batch_size_test=2\n",
    "num_worker=1\n",
    "resume = None\n",
    "logdir = './output/'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=num_worker)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size_test, \n",
    "                         shuffle=False, num_workers=num_worker)\n",
    "\n",
    "# bts class\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "\n",
    "print('==> Making model..')\n",
    "\n",
    "#net = resnet()\n",
    "net = se_resnet18()\n",
    "net = net.to(device)\n",
    "num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('The number of parameters of model is', num_params)\n",
    "# print(net)\n",
    "\n",
    "if resume is not None:\n",
    "    checkpoint = torch.load('./save_model/' + str(exp_id))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma=2.0).to(device)\n",
    "#criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, \n",
    "#                      eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "decay_epoch = [2400, 3600]\n",
    "step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, \n",
    "                                 milestones=decay_epoch, gamma=0.1)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "\n",
    "def train(epoch, global_steps):\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        global_steps += 1\n",
    "        step_lr_scheduler.step()\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    acc = 100 * correct / total\n",
    "    print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(train_loader), train_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/train error', 100 - acc, global_steps)\n",
    "    return global_steps\n",
    "\n",
    "\n",
    "def test(epoch, best_acc, global_steps):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print('test epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(test_loader), test_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/test error', 100 - acc, global_steps)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        print('==> Saving model..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('save_model'):\n",
    "            os.mkdir('save_model')\n",
    "        torch.save(state, './save_model/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    best_acc = 0\n",
    "    epoch = 0\n",
    "    global_steps = 0\n",
    "    \n",
    "    if resume is not None:\n",
    "        test(epoch=0, best_acc=0)\n",
    "    else:\n",
    "        while True:\n",
    "            epoch += 1\n",
    "            global_steps = train(epoch, global_steps)\n",
    "            best_acc = test(epoch, best_acc, global_steps)\n",
    "            print('best test accuracy is ', best_acc)\n",
    "            \n",
    "            if global_steps >= 4800:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set resnet models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# code from https://github.com/KellerJordan/ResNet-PyTorch-CIFAR10/blob/master/model.py\n",
    "class IdentityPadding(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(IdentityPadding, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "        self.add_channels = out_channels - in_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))\n",
    "        out = self.pooling(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.down_sample = down_sample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False) \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        if down_sample:\n",
    "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
    "        else:\n",
    "            self.down_sample = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        if self.down_sample is not None:\n",
    "            shortcut = self.down_sample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, num_classes=7):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # feature map size = 112x112x16\n",
    "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
    "        # feature map size = 56x56x32\n",
    "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
    "        # feature map size = 28x28x64\n",
    "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
    "\n",
    "        # output layers\n",
    "        self.avg_pool = nn.MaxPool2d(28, stride=1)\n",
    "        self.fc_out = nn.Linear(64, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_layers(self, block, in_channels, out_channels, stride):\n",
    "        if stride == 2:\n",
    "            down_sample = True\n",
    "        else:\n",
    "            down_sample = False\n",
    "        \n",
    "        layers_list = nn.ModuleList(\n",
    "            [block(in_channels, out_channels, stride, down_sample)])\n",
    "            \n",
    "        for _ in range(self.num_layers - 1):\n",
    "            layers_list.append(block(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layers_2n(x)\n",
    "        x = self.layers_4n(x)\n",
    "        x = self.layers_6n(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet():\n",
    "    block = ResidualBlock\n",
    "    # total number of layers if 6n + 2. if n is 5 then the depth of network is 32.\n",
    "    model = ResNet(3, block) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SE Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/moskomule/senet.pytorch/blob/master/senet/se_resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class SEBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.se = SELayer(planes, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None,\n",
    "                 *, reduction=16):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se = SELayer(planes * 4, reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.se(out)\n",
    "        \n",
    "\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def se_resnet18(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet34(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet50(num_classes=7, pretrained=False):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(load_state_dict_from_url(\n",
    "            \"https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet101(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def se_resnet152(num_classes=7):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "print('==> Preparing data..')\n",
    "\n",
    "cropped_image_list = []\n",
    "label_list = []\n",
    "img_path = './cropped/'\n",
    "\n",
    "with open('result.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomCrop(112, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "#number of train set and test set\n",
    "num_data=len(data.index)\n",
    "num_test_data = int(num_data/10)\n",
    "num_train_data = num_data - num_test_data\n",
    "\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "class_dict = {'JM': 0,'JN':1, 'JH':2, 'JK':3, 'RM':4, 'VV':5, 'SG':6 }\n",
    "  \n",
    "for i in data.index:\n",
    "    img = Image.open(img_path+data[0][i])\n",
    "    cropped_image_list.append(img)\n",
    "    \n",
    "    label = class_dict[data[1][i]]\n",
    "    label_list.append(label)\n",
    "    \n",
    "\n",
    "#dataset making by random select\n",
    "dataset_train = []\n",
    "dataset_test = []\n",
    "\n",
    "#random select test data index\n",
    "rand = np.random.choice(np.arange(num_data), num_test_data, replace=False)\n",
    "\n",
    "# class dataset() and __init__() looks good for calling and managing self variables\n",
    "# but This time, tried to make simple without class\n",
    "\n",
    "count_test=0\n",
    "count_train=0\n",
    "for i in np.arange(num_data):\n",
    "    if i in rand:\n",
    "        img = transforms_test(cropped_image_list[i])\n",
    "        dataset_test.append([img,label_list[i]])\n",
    "    else:\n",
    "        img = transforms_train(cropped_image_list[i])\n",
    "        dataset_train.append([img,label_list[i]])\n",
    "        \n",
    "#dataset_test = np.asarray(dataset_test)\n",
    "#dataset_train = np.asarray(dataset_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal loss\n",
    "https://github.com/foamliu/InsightFace-v2/blob/master/focal_loss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "exp_id=0\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "batch_size_test=2\n",
    "num_worker=1\n",
    "resume = None\n",
    "logdir = './output/'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=num_worker)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size_test, \n",
    "                         shuffle=False, num_workers=num_worker)\n",
    "\n",
    "# bts class\n",
    "classes = ('JM', 'JN', 'JH', 'JK', 'RM', 'VV', 'SG')\n",
    "\n",
    "print('==> Making model..')\n",
    "\n",
    "#net = resnet()\n",
    "net = se_resnet18()\n",
    "net = net.to(device)\n",
    "num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print('The number of parameters of model is', num_params)\n",
    "# print(net)\n",
    "\n",
    "if resume is not None:\n",
    "    checkpoint = torch.load('./save_model/' + str(exp_id))\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = FocalLoss(gamma=2.0).to(device)\n",
    "#criterion = nn.TripletMarginLoss(margin=1.0, p=2.0, \n",
    "#                      eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "decay_epoch = [2400, 3600]\n",
    "step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, \n",
    "                                 milestones=decay_epoch, gamma=0.1)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "\n",
    "def train(epoch, global_steps):\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        global_steps += 1\n",
    "        step_lr_scheduler.step()\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "    acc = 100 * correct / total\n",
    "    print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(train_loader), train_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/train error', 100 - acc, global_steps)\n",
    "    return global_steps\n",
    "\n",
    "\n",
    "def test(epoch, best_acc, global_steps):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    acc = 100 * correct / total\n",
    "    print('test epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(\n",
    "           epoch, batch_idx, len(test_loader), test_loss/(batch_idx+1), acc))\n",
    "\n",
    "    writer.add_scalar('./log/test error', 100 - acc, global_steps)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        print('==> Saving model..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('save_model'):\n",
    "            os.mkdir('save_model')\n",
    "        torch.save(state, './save_model/ckpt.pth')\n",
    "        best_acc = acc\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    best_acc = 0\n",
    "    epoch = 0\n",
    "    global_steps = 0\n",
    "    \n",
    "    if resume is not None:\n",
    "        test(epoch=0, best_acc=0)\n",
    "    else:\n",
    "        while True:\n",
    "            epoch += 1\n",
    "            global_steps = train(epoch, global_steps)\n",
    "            best_acc = test(epoch, best_acc, global_steps)\n",
    "            print('best test accuracy is ', best_acc)\n",
    "            \n",
    "            if global_steps >= 4800:\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
